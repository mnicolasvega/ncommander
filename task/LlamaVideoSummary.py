from service.ModelFactory import ModelFactory
from service.PromptService import PromptService
from task.BaseTask import BaseTask
from task.exception import LocalLLMError
from typing import Any, Dict, List
import html
import json
import os

class LlamaVideoSummary(BaseTask):
    def __init__(self) -> None:
        super().__init__()
        self._prompt_service = PromptService()
        self._model_factory = ModelFactory(print_fn=self._print)

    def run(self, carry: Dict[str, Any]) -> Dict[str, Any]:
        try:
            dir_path = carry.get('dir_path', '').strip()
            if not dir_path:
                raise LocalLLMError("dir_path is required")
            if not os.path.isabs(dir_path):
                raise LocalLLMError("dir_path must be an absolute path")
            if not os.path.exists(dir_path):
                raise LocalLLMError(f"Directory does not exist: {dir_path}")
            # Find all JSON files generated by WhisperSubtitleTask
            json_files = self._find_json_files(dir_path)
            if not json_files:
                self._print("No JSON subtitle files found")
                return {"processed": 0, "skipped": 0, "failed": 0, "results": []}
            llm = self._model_factory.get_model(carry)
            processed = 0
            skipped = 0
            failed = 0
            results = []
            for json_file in json_files:
                try:
                    parsed_json_path = self._derive_parsed_json_path(json_file)
                    csv_content = self._json_to_csv_string(json_file)
                    video_title = os.path.splitext(os.path.basename(json_file))[0]
                    self._print(f"Processing {os.path.basename(json_file)}...")
                    corrected_csv = self._correct_with_llm(csv_content, video_title, llm, carry)
                    corrected_json = self._csv_string_to_json(corrected_csv)
                    self._save_json(parsed_json_path, corrected_json)
                    processed += 1
                    results.append({
                        "file": json_file,
                        "status": "success",
                        "parsed_json_path": parsed_json_path
                    })
                    self._print(f"✓ Processed {os.path.basename(json_file)}")
                except Exception as e:
                    failed += 1
                    error_msg = str(e)
                    self._print(f"✗ Failed {os.path.basename(json_file)}: {error_msg}")
                    results.append({
                        "file": json_file,
                        "status": "failed",
                        "error": error_msg
                    })
            return {
                "dir_path": dir_path,
                "processed": processed,
                "skipped": skipped,
                "failed": failed,
                "results": results
            }
            
        except LocalLLMError as e:
            self._print(f"Error: {str(e)}")
            return {"error": str(e)}
        except Exception as e:
            import traceback
            self._print(f"Error: {str(e)}")
            self._print(f"Traceback: {traceback.format_exc()}")
            return {"error": str(e)}

    def text_output(self, data: Dict[str, Any]) -> str:
        if 'error' in data:
            return f"error: {data['error']}"
        processed = data.get('processed', 0)
        skipped = data.get('skipped', 0)
        failed = data.get('failed', 0)
        return f"Video Summary Correction: {processed} processed, {skipped} skipped, {failed} failed"

    def html_output(self, data: Dict[str, Any]) -> str:
        if 'error' in data:
            return self._render_html_from_template('template/LlamaVideoSummaryError.html', {
                'error_message': html.escape(str(data['error']))
            })
        results = data.get('results', [])
        items_html = []
        for result in results:
            file_name = os.path.basename(result.get('file', ''))
            status = result.get('status', 'unknown')
            item_html = self._render_html_from_template('template/LlamaVideoSummaryItem.html', {
                'file_name': html.escape(file_name),
                'status': html.escape(status),
                'parsed_json_path': html.escape(result.get('parsed_json_path', '')),
                'error': html.escape(result.get('error', ''))
            })
            items_html.append(item_html)
        return self._render_html_from_template('template/LlamaVideoSummary.html', {
            'dir_path': html.escape(data.get('dir_path', '')),
            'processed': data.get('processed', 0),
            'skipped': data.get('skipped', 0),
            'failed': data.get('failed', 0),
            'items': '\n'.join(items_html)
        })

    def name(self) -> str:
        return "llama_video_summary"

    def interval(self) -> int:
        return 60 * 60

    def cpus(self) -> float:
        return 10.0

    def memory_gb(self) -> float:
        return 10.0

    def dependencies(self) -> Dict[str, Any]:
        return {
            "pip": [
                "llama-cpp-python==0.3.2",
            ],
            "other": [
                "cmake",
                "build-essential",
                "libopenblas-dev",
            ],
        }

    def volumes(self, params: Dict[str, Any]) -> Dict[str, Dict[str, str]]:
        task_dir = os.path.dirname(os.path.abspath(__file__))
        commander_dir = os.path.dirname(task_dir)
        models_dir = os.path.join(commander_dir, 'lib', 'model')
        volumes = {
            models_dir: {
                "bind": "/app/lib/model",
                "mode": "rw",
            }
        }
        dir_path = str(params.get("dir_path", "")).strip()
        if dir_path and os.path.isabs(dir_path) and os.path.exists(dir_path):
            volumes[dir_path] = {
                "bind": "/mnt/video_input",
                "mode": "rw",
            }
        return volumes

    def ports(self, params: Dict[str, Any]) -> Dict[int, int]:
        return {}

    def requires_connection(self) -> bool:
        return True

    def max_time_expected(self) -> float | None:
        return None

    def _find_json_files(self, root_dir: str) -> List[str]:
        """Find all JSON subtitle files that don't have an associated parsed JSON."""
        json_files = []
        for current_root, _, files in os.walk(root_dir):
            for f in files:
                if f.endswith('.json') and not f.endswith('parsed.json'):
                    json_path = os.path.join(current_root, f)
                    parsed_json_path = self._derive_parsed_json_path(json_path)
                    # Only include if parsed JSON doesn't exist
                    if not os.path.exists(parsed_json_path):
                        json_files.append(json_path)
        return sorted(json_files)

    def _derive_parsed_json_path(self, json_path: str) -> str:
        """Derive parsed JSON path from JSON path."""
        base, _ = os.path.splitext(json_path)
        return f"{base} parsed.json"

    def _json_to_csv_string(self, json_path: str) -> str:
        """Convert JSON subtitle file to CSV string format for LLM prompt."""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Return CSV content as string for LLM prompt
        csv_lines = ['timestamp,text']
        for timestamp, text in sorted(data.items(), key=lambda x: int(x[0])):
            # Escape quotes in text for CSV format
            text_escaped = text.replace('"', '""')
            csv_lines.append(f'{timestamp},"{text_escaped}"')
        
        return '\n'.join(csv_lines)

    def _correct_with_llm(self, csv_content: str, context: str, llm, carry: Dict[str, Any]) -> str:
        """Use LLM to correct the CSV content."""
        # Load prompt template from file
        task_dir = os.path.dirname(os.path.abspath(__file__))
        prompt_path = os.path.join(task_dir, 'video_summary_correction.md')
        
        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompt_template = f.read()
        
        prompt = prompt_template.format(context=context, csv_content=csv_content)

        max_tokens = int(carry.get('max_tokens', 2048))
        temperature = float(carry.get('temperature', 0.2))
        top_p = float(carry.get('top_p', 0.95))
        
        formatted_prompt = self._prompt_service.get_formatted_prompt(prompt)
        
        result = llm.create_completion(
            prompt=formatted_prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        
        response = result.get('choices', [{}])[0].get('text', '').strip()
        return response

    def _csv_string_to_json(self, csv_content: str) -> Dict[int, str]:
        """Convert CSV string back to JSON format."""
        lines = csv_content.strip().split('\n')
        result = {}
        for line in lines[1:]:  # Skip header
            if ',' in line:
                parts = line.split(',', 1)
                if len(parts) == 2:
                    timestamp = parts[0].strip()
                    text = parts[1].strip().strip('"')
                    if timestamp.isdigit():
                        result[int(timestamp)] = text
        return result

    def _save_json(self, json_path: str, data: Dict[int, str]) -> None:
        """Save corrected JSON content to file."""
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
