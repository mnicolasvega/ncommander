from service.ModelFactory import ModelFactory
from service.PromptService import PromptService
from task.BaseTask import BaseTask
from task.exception import LocalLLMError
from typing import Any, Dict, List
import html
import json
import os

class LlamaVideoSummary(BaseTask):
    def __init__(self) -> None:
        super().__init__()
        self._prompt_service = PromptService()
        self._model_factory = ModelFactory(print_fn=self._print)

    def run(self, carry: Dict[str, Any]) -> Dict[str, Any]:
        try:
            dir_path = carry.get('dir_path', '').strip()
            if not dir_path:
                raise LocalLLMError("dir_path is required")
            if not os.path.isabs(dir_path):
                raise LocalLLMError("dir_path must be an absolute path")
            if not os.path.exists(dir_path):
                raise LocalLLMError(f"Directory does not exist: {dir_path}")
            # Find all JSON files generated by WhisperSubtitleTask
            json_files = self._find_json_files(dir_path)
            if not json_files:
                self._print("No JSON subtitle files found")
                return {"processed": 0, "skipped": 0, "failed": 0, "results": []}
            llm = self._model_factory.get_model(carry)
            processed = 0
            skipped = 0
            failed = 0
            results = []
            for json_file in json_files:
                try:
                    parsed_json_path = self._derive_parsed_json_path(json_file)
                    # Load original JSON
                    with open(json_file, 'r', encoding='utf-8') as f:
                        original_json = json.load(f)
                    video_title = os.path.splitext(os.path.basename(json_file))[0]
                    self._print(f"Processing {os.path.basename(json_file)}...")
                    # Correct text values while keeping timestamps
                    corrected_json = self._correct_json_with_llm(original_json, video_title, llm, carry)
                    self._save_json(parsed_json_path, corrected_json)
                    processed += 1
                    results.append({
                        "file": json_file,
                        "status": "success",
                        "parsed_json_path": parsed_json_path
                    })
                    self._print(f"✓ Processed {os.path.basename(json_file)}")
                except Exception as e:
                    failed += 1
                    error_msg = str(e)
                    self._print(f"✗ Failed {os.path.basename(json_file)}: {error_msg}")
                    results.append({
                        "file": json_file,
                        "status": "failed",
                        "error": error_msg
                    })
            return {
                "dir_path": dir_path,
                "processed": processed,
                "skipped": skipped,
                "failed": failed,
                "results": results
            }
            
        except LocalLLMError as e:
            self._print(f"Error: {str(e)}")
            return {"error": str(e)}
        except Exception as e:
            import traceback
            self._print(f"Error: {str(e)}")
            self._print(f"Traceback: {traceback.format_exc()}")
            return {"error": str(e)}

    def text_output(self, data: Dict[str, Any]) -> str:
        if 'error' in data:
            return f"error: {data['error']}"
        processed = data.get('processed', 0)
        skipped = data.get('skipped', 0)
        failed = data.get('failed', 0)
        return f"Video Summary Correction: {processed} processed, {skipped} skipped, {failed} failed"

    def html_output(self, data: Dict[str, Any]) -> str:
        if 'error' in data:
            return self._render_html_from_template('template/LlamaVideoSummaryError.html', {
                'error_message': html.escape(str(data['error']))
            })
        results = data.get('results', [])
        items_html = []
        for result in results:
            file_name = os.path.basename(result.get('file', ''))
            status = result.get('status', 'unknown')
            parsed_json_path = result.get('parsed_json_path', '')
            # Read parsed JSON content if available
            parsed_json_content = ''
            if status == 'success' and parsed_json_path and os.path.exists(parsed_json_path):
                try:
                    with open(parsed_json_path, 'r', encoding='utf-8') as f:
                        parsed_data = json.load(f)
                    # Format as table rows
                    rows = []
                    for timestamp, text in sorted(parsed_data.items(), key=lambda x: int(x[0])):
                        rows.append(f'<tr><td>{html.escape(timestamp)}</td><td>{html.escape(text)}</td></tr>')
                    parsed_json_content = '\n'.join(rows)
                except Exception:
                    parsed_json_content = ''
            item_html = self._render_html_from_template('template/LlamaVideoSummaryItem.html', {
                'file_name': html.escape(file_name),
                'status': html.escape(status),
                'parsed_json_path': html.escape(parsed_json_path),
                'parsed_json_content': parsed_json_content,
                'error': html.escape(result.get('error', ''))
            })
            items_html.append(item_html)
        return self._render_html_from_template('template/LlamaVideoSummary.html', {
            'dir_path': html.escape(data.get('dir_path', '')),
            'processed': data.get('processed', 0),
            'skipped': data.get('skipped', 0),
            'failed': data.get('failed', 0),
            'items': '\n'.join(items_html)
        })

    def name(self) -> str:
        return "llama_video_summary"

    def interval(self) -> int:
        return 60 * 60

    def cpus(self) -> float:
        return 20.0

    def memory_gb(self) -> float:
        return 20.0

    def dependencies(self) -> Dict[str, Any]:
        return {
            "pip": [
                "llama-cpp-python==0.3.2",
            ],
            "other": [
                "cmake",
                "build-essential",
                "libopenblas-dev",
            ],
        }

    def volumes(self, params: Dict[str, Any]) -> Dict[str, Dict[str, str]]:
        task_dir = os.path.dirname(os.path.abspath(__file__))
        commander_dir = os.path.dirname(task_dir)
        models_dir = os.path.join(commander_dir, 'lib', 'model')
        volumes = {
            models_dir: {
                "bind": "/app/lib/model",
                "mode": "rw",
            }
        }
        dir_path = str(params.get("dir_path", "")).strip()
        if dir_path and os.path.isabs(dir_path) and os.path.exists(dir_path):
            volumes[dir_path] = {
                "bind": "/mnt/video_input",
                "mode": "rw",
            }
        return volumes

    def ports(self, params: Dict[str, Any]) -> Dict[int, int]:
        return {}

    def requires_connection(self) -> bool:
        return True

    def max_time_expected(self) -> float | None:
        return None

    def _find_json_files(self, root_dir: str) -> List[str]:
        """Find all JSON subtitle files."""
        json_files = []
        for current_root, _, files in os.walk(root_dir):
            for f in files:
                if f.endswith('.json') and not f.endswith('parsed.json'):
                    json_path = os.path.join(current_root, f)
                    json_files.append(json_path)
        return sorted(json_files)

    def _derive_parsed_json_path(self, json_path: str) -> str:
        """Derive parsed JSON path from JSON path."""
        base, _ = os.path.splitext(json_path)
        return f"{base} parsed.json"

    def _correct_json_with_llm(self, json_data: Dict[str, str], context: str, llm, carry: Dict[str, Any]) -> Dict[str, str]:
        """Use LLM to correct text values in the JSON dict."""
        # Convert JSON to a format the LLM can process
        json_str = json.dumps(json_data, ensure_ascii=False, indent=2)
        self._print(f"Input JSON has {len(json_data)} entries, {len(json_str)} chars")
        
        # Load prompt template
        task_dir = os.path.dirname(os.path.abspath(__file__))
        prompt_path = os.path.join(task_dir, 'video_summary_correction.md')
        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompt_template = f.read()
        
        prompt = prompt_template.format(context=context, json_content=json_str)
        response_text = self._get_llm_response(prompt, llm, carry)
        try:
            # Remove markdown code blocks if present
            cleaned_response = response_text
            if response_text.startswith('```'):
                self._print("Detected markdown code block, removing...")
                lines = response_text.split('\n')
                cleaned_response = '\n'.join(lines[1:-1]) if len(lines) > 2 else response_text
                self._print(f"Cleaned response preview: {cleaned_response[:500]}")
            corrected_json = json.loads(cleaned_response)
            self._print(f"Successfully parsed JSON with {len(corrected_json)} entries")
            return corrected_json
        except json.JSONDecodeError as e:
            # If parsing fails, return original
            self._print(f"ERROR: Failed to parse LLM response as JSON")
            self._print(f"JSON Error: {str(e)}")
            self._print(f"Failed text (first 1000 chars): {response_text[:1000]}")
            self._print(f"Failed text (last 500 chars): {response_text[-500:]}")
            return json_data

    def _get_llm_response(self, prompt: str, llm, carry: Dict[str, Any]) -> str:
        self._print(f"Prompt length: {len(prompt)} chars")
        max_tokens = int(carry.get('max_tokens', 2048))
        temperature = float(carry.get('temperature', 0.2))
        top_p = float(carry.get('top_p', 0.95))
        self._print(f"LLM params: max_tokens={max_tokens}, temperature={temperature}, top_p={top_p}")
        self._print(f"Sending prompt directly (length: {len(prompt)} chars)")
        result = llm.create_completion(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        response_text = result.get('choices', [{}])[0].get('text', '').strip()
        self._print(f"LLM Response length: {len(response_text)} chars")
        self._print(f"LLM Response preview (first 500 chars): {response_text[:500]}")
        return response_text

    def _save_json(self, json_path: str, data: Dict[str, str]) -> None:
        """Save corrected JSON content to file."""
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
